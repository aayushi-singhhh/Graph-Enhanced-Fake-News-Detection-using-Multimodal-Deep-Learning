{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6451efbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>ord_in_thread</th>\n",
       "      <th>author</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>crawled</th>\n",
       "      <th>site_url</th>\n",
       "      <th>country</th>\n",
       "      <th>domain_rank</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>spam_score</th>\n",
       "      <th>main_img_url</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>participants_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>shares</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6a175f46bcd24d39b3e962ad0f29936721db70db</td>\n",
       "      <td>0</td>\n",
       "      <td>Barracuda Brigade</td>\n",
       "      <td>2016-10-26T21:41:00.000+03:00</td>\n",
       "      <td>Muslims BUSTED: They Stole Millions In Gov‚Äôt B...</td>\n",
       "      <td>Print They should pay all the back all the mon...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-10-27T01:49:27.168+03:00</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>US</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>Muslims BUSTED: They Stole Millions In Gov‚Äôt B...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2bdc29d12605ef9cf3f09f9875040a7113be5d5b</td>\n",
       "      <td>0</td>\n",
       "      <td>reasoning with facts</td>\n",
       "      <td>2016-10-29T08:47:11.259+03:00</td>\n",
       "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
       "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-10-29T08:47:11.259+03:00</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>US</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c70e149fdd53de5e61c29281100b9de0ed268bc3</td>\n",
       "      <td>0</td>\n",
       "      <td>Barracuda Brigade</td>\n",
       "      <td>2016-10-31T01:41:49.479+02:00</td>\n",
       "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
       "      <td>Red State : \\nFox News Sunday reported this mo...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-10-31T01:41:49.479+02:00</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>US</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>http://bb4sp.com/wp-content/uploads/2016/10/Fu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7cf7c15731ac2a116dd7f629bd57ea468ed70284</td>\n",
       "      <td>0</td>\n",
       "      <td>Fed Up</td>\n",
       "      <td>2016-11-01T05:22:00.000+02:00</td>\n",
       "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
       "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-11-01T15:46:26.304+02:00</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>US</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
       "      <td>0.068</td>\n",
       "      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0206b54719c7e241ffe0ad4315b808290dbe6c0f</td>\n",
       "      <td>0</td>\n",
       "      <td>Fed Up</td>\n",
       "      <td>2016-11-01T21:56:00.000+02:00</td>\n",
       "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
       "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
       "      <td>english</td>\n",
       "      <td>2016-11-01T23:59:42.266+02:00</td>\n",
       "      <td>100percentfedup.com</td>\n",
       "      <td>US</td>\n",
       "      <td>25689.0</td>\n",
       "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
       "      <td>0.865</td>\n",
       "      <td>http://100percentfedup.com/wp-content/uploads/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       uuid  ord_in_thread  \\\n",
       "0  6a175f46bcd24d39b3e962ad0f29936721db70db              0   \n",
       "1  2bdc29d12605ef9cf3f09f9875040a7113be5d5b              0   \n",
       "2  c70e149fdd53de5e61c29281100b9de0ed268bc3              0   \n",
       "3  7cf7c15731ac2a116dd7f629bd57ea468ed70284              0   \n",
       "4  0206b54719c7e241ffe0ad4315b808290dbe6c0f              0   \n",
       "\n",
       "                 author                      published  \\\n",
       "0     Barracuda Brigade  2016-10-26T21:41:00.000+03:00   \n",
       "1  reasoning with facts  2016-10-29T08:47:11.259+03:00   \n",
       "2     Barracuda Brigade  2016-10-31T01:41:49.479+02:00   \n",
       "3                Fed Up  2016-11-01T05:22:00.000+02:00   \n",
       "4                Fed Up  2016-11-01T21:56:00.000+02:00   \n",
       "\n",
       "                                               title  \\\n",
       "0  Muslims BUSTED: They Stole Millions In Gov‚Äôt B...   \n",
       "1  Re: Why Did Attorney General Loretta Lynch Ple...   \n",
       "2  BREAKING: Weiner Cooperating With FBI On Hilla...   \n",
       "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
       "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...   \n",
       "\n",
       "                                                text language  \\\n",
       "0  Print They should pay all the back all the mon...  english   \n",
       "1  Why Did Attorney General Loretta Lynch Plead T...  english   \n",
       "2  Red State : \\nFox News Sunday reported this mo...  english   \n",
       "3  Email Kayla Mueller was a prisoner and torture...  english   \n",
       "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...  english   \n",
       "\n",
       "                         crawled             site_url country  domain_rank  \\\n",
       "0  2016-10-27T01:49:27.168+03:00  100percentfedup.com      US      25689.0   \n",
       "1  2016-10-29T08:47:11.259+03:00  100percentfedup.com      US      25689.0   \n",
       "2  2016-10-31T01:41:49.479+02:00  100percentfedup.com      US      25689.0   \n",
       "3  2016-11-01T15:46:26.304+02:00  100percentfedup.com      US      25689.0   \n",
       "4  2016-11-01T23:59:42.266+02:00  100percentfedup.com      US      25689.0   \n",
       "\n",
       "                                        thread_title  spam_score  \\\n",
       "0  Muslims BUSTED: They Stole Millions In Gov‚Äôt B...       0.000   \n",
       "1  Re: Why Did Attorney General Loretta Lynch Ple...       0.000   \n",
       "2  BREAKING: Weiner Cooperating With FBI On Hilla...       0.000   \n",
       "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...       0.068   \n",
       "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...       0.865   \n",
       "\n",
       "                                        main_img_url  replies_count  \\\n",
       "0  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
       "1  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
       "2  http://bb4sp.com/wp-content/uploads/2016/10/Fu...              0   \n",
       "3  http://100percentfedup.com/wp-content/uploads/...              0   \n",
       "4  http://100percentfedup.com/wp-content/uploads/...              0   \n",
       "\n",
       "   participants_count  likes  comments  shares  type  \n",
       "0                   1      0         0       0  bias  \n",
       "1                   1      0         0       0  bias  \n",
       "2                   1      0         0       0  bias  \n",
       "3                   0      0         0       0  bias  \n",
       "4                   0      0         0       0  bias  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/fake.csv\")  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cebba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12999 entries, 0 to 12998\n",
      "Data columns (total 20 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   uuid                12999 non-null  object \n",
      " 1   ord_in_thread       12999 non-null  int64  \n",
      " 2   author              10575 non-null  object \n",
      " 3   published           12999 non-null  object \n",
      " 4   title               12319 non-null  object \n",
      " 5   text                12953 non-null  object \n",
      " 6   language            12999 non-null  object \n",
      " 7   crawled             12999 non-null  object \n",
      " 8   site_url            12999 non-null  object \n",
      " 9   country             12823 non-null  object \n",
      " 10  domain_rank         8776 non-null   float64\n",
      " 11  thread_title        12987 non-null  object \n",
      " 12  spam_score          12999 non-null  float64\n",
      " 13  main_img_url        9356 non-null   object \n",
      " 14  replies_count       12999 non-null  int64  \n",
      " 15  participants_count  12999 non-null  int64  \n",
      " 16  likes               12999 non-null  int64  \n",
      " 17  comments            12999 non-null  int64  \n",
      " 18  shares              12999 non-null  int64  \n",
      " 19  type                12999 non-null  object \n",
      "dtypes: float64(2), int64(6), object(12)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Count missing values in each column\n",
    "df.isnull().sum()\n",
    "# Shape of dataset (rows, columns)\n",
    "df.shape\n",
    "# Column info and datatypes\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d54f29",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Graph-Enhanced-Fake-News-Detection-using-Multimodal-Deep-Learning/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Count how many fake vs real\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Visualize distribution\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Graph-Enhanced-Fake-News-Detection-using-Multimodal-Deep-Learning/.venv/lib/python3.9/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/Graph-Enhanced-Fake-News-Detection-using-Multimodal-Deep-Learning/.venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "# Count how many fake vs real\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Visualize distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['label'].value_counts().plot(kind='bar', title='Label Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3666087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (12999, 20)\n",
      "Original type distribution:\n",
      "type\n",
      "bs            11492\n",
      "bias            443\n",
      "conspiracy      430\n",
      "hate            246\n",
      "satire          146\n",
      "state           121\n",
      "junksci         102\n",
      "fake             19\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "\n",
      "After selecting columns: (12999, 3)\n",
      "Missing values before cleaning:\n",
      "title    680\n",
      "text      46\n",
      "type       0\n",
      "dtype: int64\n",
      "After dropping missing values: (12273, 3)\n",
      "After combining title and text: (12273, 4)\n",
      "\n",
      "Final binary label distribution:\n",
      "label\n",
      "1    11140\n",
      "0     1133\n",
      "Name: count, dtype: int64\n",
      "Fake (0): 1133\n",
      "Real (1): 11140\n",
      "\n",
      "Final dataset shape: (12273, 2)\n",
      "\n",
      "First 5 rows:\n",
      "                                             content  label\n",
      "0  Muslims BUSTED: They Stole Millions In Gov‚Äôt B...      0\n",
      "1  Re: Why Did Attorney General Loretta Lynch Ple...      0\n",
      "2  BREAKING: Weiner Cooperating With FBI On Hilla...      0\n",
      "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...      0\n",
      "4  FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...      0\n"
     ]
    }
   ],
   "source": [
    "print(\"Original dataset shape:\", df.shape)\n",
    "print(\"Original type distribution:\")\n",
    "print(df['type'].value_counts())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 1: Keep only 'title', 'text', and 'type' columns\n",
    "df_clean = df[['title', 'text', 'type']].copy()\n",
    "print(\"After selecting columns:\", df_clean.shape)\n",
    "\n",
    "# Step 2: Drop rows with missing values\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df_clean.isnull().sum())\n",
    "df_clean = df_clean.dropna()\n",
    "print(\"After dropping missing values:\", df_clean.shape)\n",
    "\n",
    "# Step 3: Combine 'title' and 'text' into a single column called 'content'\n",
    "df_clean['content'] = df_clean['title'].astype(str) + ' ' + df_clean['text'].astype(str)\n",
    "print(\"After combining title and text:\", df_clean.shape)\n",
    "\n",
    "# Step 4: Convert labels in 'type' to numeric (fake=0, real=1)\n",
    "# Based on the data exploration, we need to map the categories to binary classification\n",
    "# Let's consider 'fake', 'bias', 'conspiracy', 'hate', 'junksci' as fake (0)\n",
    "# and 'bs', 'satire', 'state' as real (1) - though this may need adjustment based on your specific needs\n",
    "\n",
    "fake_categories = ['fake', 'bias', 'conspiracy', 'hate', 'junksci']\n",
    "real_categories = ['bs', 'satire', 'state']\n",
    "\n",
    "def map_to_binary(type_val):\n",
    "    if type_val in fake_categories:\n",
    "        return 0  # fake\n",
    "    elif type_val in real_categories:\n",
    "        return 1  # real\n",
    "    else:\n",
    "        return -1  # unknown category\n",
    "\n",
    "df_clean['label'] = df_clean['type'].apply(map_to_binary)\n",
    "\n",
    "# Remove any rows with unknown categories (-1)\n",
    "df_clean = df_clean[df_clean['label'] != -1]\n",
    "\n",
    "# Keep only the content and label columns for the final dataset\n",
    "df_final = df_clean[['content', 'label']].copy()\n",
    "\n",
    "print(\"\\nFinal binary label distribution:\")\n",
    "print(df_final['label'].value_counts())\n",
    "print(f\"Fake (0): {(df_final['label'] == 0).sum()}\")\n",
    "print(f\"Real (1): {(df_final['label'] == 1).sum()}\")\n",
    "\n",
    "# Step 5: Print dataset shape and first 5 rows\n",
    "print(f\"\\nFinal dataset shape: {df_final.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98261e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      "Preprocessing text data...\n",
      "Original data shape: (12273, 2)\n",
      "After preprocessing:\n",
      "Data shape: (12273, 3)\n",
      "Columns: ['content', 'label', 'clean_content']\n",
      "\n",
      "================================================================================\n",
      "BEFORE vs AFTER PREPROCESSING - 5 Examples:\n",
      "================================================================================\n",
      "\n",
      "--- Example 1 ---\n",
      "Label: 0 (Fake)\n",
      "ORIGINAL: Muslims BUSTED: They Stole Millions In Gov‚Äôt Benefits Print They should pay all the back all the money plus interest. The entire family and everyone who came in with them need to be deported asap. Why...\n",
      "CLEANED:  muslim busted stole million gov benefit print pay back money plus interest entire family everyone came need deported asap take two year bust ‚Ä¶another group stealing government taxpayer group somali st...\n",
      "\n",
      "--- Example 2 ---\n",
      "Label: 0 (Fake)\n",
      "ORIGINAL: Re: Why Did Attorney General Loretta Lynch Plead The Fifth? Why Did Attorney General Loretta Lynch Plead The Fifth? Barracuda Brigade 2016-10-28 Print The administration is blocking congressional prob...\n",
      "CLEANED:  attorney general loretta lynch plead fifth attorney general loretta lynch plead fifth barracuda brigade print administration blocking congressional probe cash payment iran course need plead either rec...\n",
      "\n",
      "--- Example 3 ---\n",
      "Label: 0 (Fake)\n",
      "ORIGINAL: BREAKING: Weiner Cooperating With FBI On Hillary Email Investigation Red State : \n",
      "Fox News Sunday reported this morning that Anthony Weiner is cooperating with the FBI, which has re-opened (yes, lefti...\n",
      "CLEANED:  breaking weiner cooperating fbi hillary email investigation red state fox news sunday reported morning anthony weiner cooperating fbi reopened yes lefty reopened investigation hillary clinton classifi...\n",
      "\n",
      "--- Example 4 ---\n",
      "Label: 0 (Fake)\n",
      "ORIGINAL: PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnapped And Killed By ISIS: \"I have voted for Donald J. Trump!\" ¬ª 100percentfedUp.com Email Kayla Mueller was a prisoner and tortured by ISIS while no chance of...\n",
      "CLEANED:  pin drop speech father daughter kidnapped killed isi voted donald trump percentfedupcom email kayla mueller prisoner tortured isi chance release‚Ä¶a horrific story father gave pin drop speech heartfelt ...\n",
      "\n",
      "--- Example 5 ---\n",
      "Label: 0 (Fake)\n",
      "ORIGINAL: FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Healthcare Begins With A Bombshell! ¬ª 100percentfedUp.com Email HEALTHCARE REFORM TO MAKE AMERICA GREAT AGAIN \n",
      "Since March of 2010, the American people have h...\n",
      "CLEANED:  fantastic trump point plan reform healthcare begin bombshell percentfedupcom email healthcare reform make america great since march american people suffer incredible economic burden affordable care ac...\n",
      "\n",
      "Rows with empty cleaned content: 0\n",
      "\n",
      "Final dataset summary:\n",
      "Total samples: 12273\n",
      "Fake samples: 1133\n",
      "Real samples: 11140\n",
      "Average cleaned content length: 2768.4 characters\n"
     ]
    }
   ],
   "source": [
    "# TODO: Preprocess text data for fake news detection\n",
    "# Steps:\n",
    "# 1. Lowercase all text in 'content'\n",
    "# 2. Remove punctuation, numbers, and extra spaces\n",
    "# 3. Remove stopwords (like \"the\", \"is\", \"and\")\n",
    "# 4. Apply lemmatization (convert words to root form, e.g., \"running\" -> \"run\")\n",
    "# 5. Store the cleaned text in a new column 'clean_content'\n",
    "# 6. Print 5 cleaned examples\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing function\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Lowercase all text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 2: Remove punctuation, numbers, and extra spaces\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Step 3 & 4: Tokenize, remove stopwords, and lemmatize\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens \n",
    "              if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Preprocessing text data...\")\n",
    "print(\"Original data shape:\", df_final.shape)\n",
    "\n",
    "# Apply preprocessing to the content column\n",
    "df_final['clean_content'] = df_final['content'].apply(preprocess_text)\n",
    "\n",
    "print(\"After preprocessing:\")\n",
    "print(\"Data shape:\", df_final.shape)\n",
    "print(\"Columns:\", df_final.columns.tolist())\n",
    "\n",
    "# Step 6: Print 5 cleaned examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEFORE vs AFTER PREPROCESSING - 5 Examples:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Label: {df_final.iloc[i]['label']} ({'Fake' if df_final.iloc[i]['label'] == 0 else 'Real'})\")\n",
    "    print(f\"ORIGINAL: {df_final.iloc[i]['content'][:200]}...\")\n",
    "    print(f\"CLEANED:  {df_final.iloc[i]['clean_content'][:200]}...\")\n",
    "\n",
    "# Check for any empty cleaned content\n",
    "empty_content = df_final['clean_content'].str.len() == 0\n",
    "print(f\"\\nRows with empty cleaned content: {empty_content.sum()}\")\n",
    "\n",
    "if empty_content.sum() > 0:\n",
    "    print(\"Removing rows with empty cleaned content...\")\n",
    "    df_final = df_final[~empty_content].reset_index(drop=True)\n",
    "    print(f\"Final data shape: {df_final.shape}\")\n",
    "\n",
    "print(f\"\\nFinal dataset summary:\")\n",
    "print(f\"Total samples: {len(df_final)}\")\n",
    "print(f\"Fake samples: {(df_final['label'] == 0).sum()}\")\n",
    "print(f\"Real samples: {(df_final['label'] == 1).sum()}\")\n",
    "print(f\"Average cleaned content length: {df_final['clean_content'].str.len().mean():.1f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb44b3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: Build a Fake News Detection pipeline step by step.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Dataset: Already preprocessed to include 'content' (title+text) and 'label' (0=fake, 1=real).\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# TODO: Build a Fake News Detection pipeline step by step.\n",
    "# Dataset: Already preprocessed to include 'content' (title+text) and 'label' (0=fake, 1=real).\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FAKE NEWS DETECTION PIPELINE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f829e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Text Cleaning (Already done in previous preprocessing step)\n",
    "print(\"\\nStep 1: Text Cleaning\")\n",
    "print(\"-\" * 40)\n",
    "print(\"‚úì Text cleaning already completed in previous step\")\n",
    "print(f\"Dataset shape: {df_final.shape}\")\n",
    "print(f\"Columns: {df_final.columns.tolist()}\")\n",
    "\n",
    "# Verify we have clean_content column\n",
    "if 'clean_content' in df_final.columns:\n",
    "    print(\"‚úì Clean content column available\")\n",
    "    print(\"\\nSample cleaned texts:\")\n",
    "    for i in range(3):\n",
    "        print(f\"\\nSample {i+1} (Label: {'Fake' if df_final.iloc[i]['label'] == 0 else 'Real'}):\")\n",
    "        print(f\"Original: {df_final.iloc[i]['content'][:100]}...\")\n",
    "        print(f\"Cleaned:  {df_final.iloc[i]['clean_content'][:100]}...\")\n",
    "else:\n",
    "    print(\"‚ùå Clean content column not found. Using 'content' column.\")\n",
    "    df_final['clean_content'] = df_final['content']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb5b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train-Test Split\n",
    "print(\"Step 2: Train-Test Split\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Use the clean_content for modeling\n",
    "X = df_final['clean_content']\n",
    "y = df_final['label']\n",
    "\n",
    "# Split into train (80%) and test (20%) sets with stratification\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Total dataset size: {len(df_final)}\")\n",
    "print(f\"Training set size: {len(X_train_text)}\")\n",
    "print(f\"Test set size: {len(X_test_text)}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nOriginal dataset distribution:\")\n",
    "print(f\"Fake (0): {(y == 0).sum()} ({(y == 0).mean()*100:.1f}%)\")\n",
    "print(f\"Real (1): {(y == 1).sum()} ({(y == 1).mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "print(f\"Fake (0): {(y_train == 0).sum()} ({(y_train == 0).mean()*100:.1f}%)\")\n",
    "print(f\"Real (1): {(y_train == 1).sum()} ({(y_train == 1).mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set distribution:\")\n",
    "print(f\"Fake (0): {(y_test == 0).sum()} ({(y_test == 0).mean()*100:.1f}%)\")\n",
    "print(f\"Real (1): {(y_test == 1).sum()} ({(y_test == 1).mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"‚úì Train-test split completed with stratification\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccca679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Extraction\n",
    "print(\"Step 3: Feature Extraction with TF-IDF\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,      # Limit vocabulary size\n",
    "    min_df=5,               # Remove very rare words (appear in less than 5 documents)\n",
    "    max_df=0.8,             # Remove very common words (appear in more than 80% of documents)\n",
    "    ngram_range=(1, 2),     # Use unigrams and bigrams\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"TF-IDF Vectorizer parameters:\")\n",
    "print(f\"- max_features: 5000\")\n",
    "print(f\"- min_df: 5 (minimum document frequency)\")\n",
    "print(f\"- max_df: 0.8 (maximum document frequency)\")\n",
    "print(f\"- ngram_range: (1, 2) (unigrams and bigrams)\")\n",
    "\n",
    "# Fit vectorizer on training data and transform both train and test\n",
    "print(\"\\nFitting TF-IDF vectorizer on training data...\")\n",
    "X_train = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_test = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f\"‚úì TF-IDF transformation completed\")\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"Feature matrix sparsity: {(1 - X_train.nnz / (X_train.shape[0] * X_train.shape[1]))*100:.2f}%\")\n",
    "\n",
    "# Show some example feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"\\nSample features: {feature_names[:20]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Baseline Model Training\n",
    "print(\"Step 4: Baseline Model Training (Logistic Regression)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Train Logistic Regression classifier\n",
    "lr_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "print(\"Training Logistic Regression model...\")\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr_model.predict(X_test)\n",
    "y_pred_proba = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úì Model training completed\")\n",
    "print(f\"\\nBaseline Model Performance:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Fake', 'Real']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Fake', 'Real'], \n",
    "           yticklabels=['Fake', 'Real'])\n",
    "plt.title('Confusion Matrix - Baseline Logistic Regression')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faece619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Handle Class Imbalance\n",
    "print(\"Step 5: Handle Class Imbalance\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Train Logistic Regression with balanced class weights\n",
    "lr_balanced = LogisticRegression(\n",
    "    random_state=RANDOM_STATE, \n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'  # Automatically adjust weights inversely proportional to class frequencies\n",
    ")\n",
    "\n",
    "print(\"Training Logistic Regression with balanced class weights...\")\n",
    "lr_balanced.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_balanced = lr_balanced.predict(X_test)\n",
    "y_pred_proba_balanced = lr_balanced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
    "precision_balanced = precision_score(y_test, y_pred_balanced)\n",
    "recall_balanced = recall_score(y_test, y_pred_balanced)\n",
    "f1_balanced = f1_score(y_test, y_pred_balanced)\n",
    "\n",
    "print(f\"‚úì Balanced model training completed\")\n",
    "\n",
    "# Compare performance\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"{'Metric':<12} {'Baseline':<10} {'Balanced':<10} {'Improvement':<12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Accuracy':<12} {accuracy:<10.4f} {accuracy_balanced:<10.4f} {accuracy_balanced-accuracy:+.4f}\")\n",
    "print(f\"{'Precision':<12} {precision:<10.4f} {precision_balanced:<10.4f} {precision_balanced-precision:+.4f}\")\n",
    "print(f\"{'Recall':<12} {recall:<10.4f} {recall_balanced:<10.4f} {recall_balanced-recall:+.4f}\")\n",
    "print(f\"{'F1-score':<12} {f1:<10.4f} {f1_balanced:<10.4f} {f1_balanced-f1:+.4f}\")\n",
    "\n",
    "# Detailed classification report for balanced model\n",
    "print(f\"\\nBalanced Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_balanced, target_names=['Fake', 'Real']))\n",
    "\n",
    "# Confusion Matrix for balanced model\n",
    "cm_balanced = confusion_matrix(y_test, y_pred_balanced)\n",
    "print(f\"\\nBalanced Model Confusion Matrix:\")\n",
    "print(cm_balanced)\n",
    "\n",
    "# Plot confusion matrices side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "           xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "ax1.set_title('Baseline Model')\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "ax1.set_ylabel('True Label')\n",
    "\n",
    "sns.heatmap(cm_balanced, annot=True, fmt='d', cmap='Greens', ax=ax2,\n",
    "           xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "ax2.set_title('Balanced Model')\n",
    "ax2.set_xlabel('Predicted Label')\n",
    "ax2.set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553aaa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Try Advanced Models\n",
    "print(\"Step 6: Advanced Models Comparison\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Dictionary to store models and their performance\n",
    "models = {}\n",
    "model_results = {}\n",
    "\n",
    "# 1. Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "models['Random Forest'] = rf_model\n",
    "model_results['Random Forest'] = {\n",
    "    'accuracy': accuracy_score(y_test, rf_pred),\n",
    "    'precision': precision_score(y_test, rf_pred),\n",
    "    'recall': recall_score(y_test, rf_pred),\n",
    "    'f1': f1_score(y_test, rf_pred)\n",
    "}\n",
    "\n",
    "# Add previous models to comparison\n",
    "models['Logistic Regression'] = lr_model\n",
    "model_results['Logistic Regression'] = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1\n",
    "}\n",
    "\n",
    "models['Logistic Regression (Balanced)'] = lr_balanced\n",
    "model_results['Logistic Regression (Balanced)'] = {\n",
    "    'accuracy': accuracy_balanced,\n",
    "    'precision': precision_balanced,\n",
    "    'recall': recall_balanced,\n",
    "    'f1': f1_balanced\n",
    "}\n",
    "\n",
    "print(\"‚úì Random Forest training completed\")\n",
    "\n",
    "# Try XGBoost if available\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    print(\"Training XGBoost...\")\n",
    "    \n",
    "    # Calculate scale_pos_weight for XGBoost class balancing\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    \n",
    "    xgb_model = XGBClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    models['XGBoost'] = xgb_model\n",
    "    model_results['XGBoost'] = {\n",
    "        'accuracy': accuracy_score(y_test, xgb_pred),\n",
    "        'precision': precision_score(y_test, xgb_pred),\n",
    "        'recall': recall_score(y_test, xgb_pred),\n",
    "        'f1': f1_score(y_test, xgb_pred)\n",
    "    }\n",
    "    print(\"‚úì XGBoost training completed\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "# Compare all models\n",
    "print(f\"\\nModel Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<30} {'Accuracy':<10} {'Precision':<12} {'Recall':<10} {'F1-Score':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "best_f1 = 0\n",
    "best_model_name = \"\"\n",
    "\n",
    "for model_name, metrics in model_results.items():\n",
    "    print(f\"{model_name:<30} {metrics['accuracy']:<10.4f} {metrics['precision']:<12.4f} \"\n",
    "          f\"{metrics['recall']:<10.4f} {metrics['f1']:<10.4f}\")\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_model_name = model_name\n",
    "\n",
    "print(f\"\\nüèÜ Best performing model: {best_model_name} (F1-Score: {best_f1:.4f})\")\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Feature importance for tree-based models\n",
    "if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "    print(f\"\\nTop 20 Most Important Features ({best_model_name}):\")\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get top features\n",
    "    top_indices = np.argsort(feature_importance)[::-1][:20]\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        print(f\"{i+1:2d}. {feature_names[idx]:<20} {feature_importance[idx]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Model Explainability\n",
    "print(\"Step 7: Model Explainability\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# For Logistic Regression, we can analyze coefficients\n",
    "if 'Logistic' in best_model_name:\n",
    "    print(f\"Analyzing {best_model_name} coefficients...\")\n",
    "    \n",
    "    # Get feature names and coefficients\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    coefficients = best_model.coef_[0]\n",
    "    \n",
    "    # Get top positive coefficients (indicating \"Real\" news)\n",
    "    top_positive_indices = np.argsort(coefficients)[::-1][:15]\n",
    "    print(f\"\\nTop 15 words indicating REAL news:\")\n",
    "    for i, idx in enumerate(top_positive_indices):\n",
    "        print(f\"{i+1:2d}. {feature_names[idx]:<20} {coefficients[idx]:.4f}\")\n",
    "    \n",
    "    # Get top negative coefficients (indicating \"Fake\" news)\n",
    "    top_negative_indices = np.argsort(coefficients)[:15]\n",
    "    print(f\"\\nTop 15 words indicating FAKE news:\")\n",
    "    for i, idx in enumerate(top_negative_indices):\n",
    "        print(f\"{i+1:2d}. {feature_names[idx]:<20} {coefficients[idx]:.4f}\")\n",
    "    \n",
    "    # Visualize top coefficients\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Top positive coefficients\n",
    "    top_pos_words = [feature_names[idx] for idx in top_positive_indices]\n",
    "    top_pos_coefs = [coefficients[idx] for idx in top_positive_indices]\n",
    "    \n",
    "    ax1.barh(range(len(top_pos_words)), top_pos_coefs, color='green', alpha=0.7)\n",
    "    ax1.set_yticks(range(len(top_pos_words)))\n",
    "    ax1.set_yticklabels(top_pos_words)\n",
    "    ax1.set_xlabel('Coefficient Value')\n",
    "    ax1.set_title('Top Words Indicating REAL News')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Top negative coefficients\n",
    "    top_neg_words = [feature_names[idx] for idx in top_negative_indices]\n",
    "    top_neg_coefs = [coefficients[idx] for idx in top_negative_indices]\n",
    "    \n",
    "    ax2.barh(range(len(top_neg_words)), top_neg_coefs, color='red', alpha=0.7)\n",
    "    ax2.set_yticks(range(len(top_neg_words)))\n",
    "    ax2.set_yticklabels(top_neg_words)\n",
    "    ax2.set_xlabel('Coefficient Value')\n",
    "    ax2.set_title('Top Words Indicating FAKE News')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"Feature importance analysis for {best_model_name}:\")\n",
    "    \n",
    "    # For tree-based models, show feature importance\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_importance = best_model.feature_importances_\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Get top and bottom features\n",
    "        top_indices = np.argsort(feature_importance)[::-1][:20]\n",
    "        \n",
    "        print(f\"\\nTop 20 Most Important Features:\")\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            print(f\"{i+1:2d}. {feature_names[idx]:<20} {feature_importance[idx]:.4f}\")\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_words = [feature_names[idx] for idx in top_indices]\n",
    "        top_importance = [feature_importance[idx] for idx in top_indices]\n",
    "        \n",
    "        plt.barh(range(len(top_words)), top_importance, color='skyblue', alpha=0.7)\n",
    "        plt.yticks(range(len(top_words)), top_words)\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title(f'Top 20 Feature Importance - {best_model_name}')\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Try SHAP if available (optional)\n",
    "try:\n",
    "    import shap\n",
    "    print(f\"\\nSHAP Analysis available. Generating sample explanations...\")\n",
    "    \n",
    "    # For demonstration, explain a few predictions\n",
    "    if 'Logistic' in best_model_name:\n",
    "        explainer = shap.LinearExplainer(best_model, X_train[:100])  # Use sample for speed\n",
    "        shap_values = explainer.shap_values(X_test[:5])\n",
    "        \n",
    "        print(\"‚úì SHAP analysis completed for sample predictions\")\n",
    "        print(\"Note: Full SHAP analysis can be computationally expensive for large datasets\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"SHAP not available. Install with: pip install shap\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Export Model and Create Prediction Function\n",
    "print(\"Step 8: Export Model and Create Prediction Function\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save the best model and vectorizer\n",
    "model_path = f'{models_dir}/best_fake_news_model.joblib'\n",
    "vectorizer_path = f'{models_dir}/tfidf_vectorizer.joblib'\n",
    "\n",
    "print(f\"Saving best model ({best_model_name}) and TF-IDF vectorizer...\")\n",
    "joblib.dump(best_model, model_path)\n",
    "joblib.dump(tfidf_vectorizer, vectorizer_path)\n",
    "\n",
    "print(f\"‚úì Model saved to: {model_path}\")\n",
    "print(f\"‚úì Vectorizer saved to: {vectorizer_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': type(best_model).__name__,\n",
    "    'performance': model_results[best_model_name],\n",
    "    'vocabulary_size': len(tfidf_vectorizer.vocabulary_),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'feature_extraction': 'TF-IDF',\n",
    "    'preprocessing_steps': [\n",
    "        'Lowercase conversion',\n",
    "        'Punctuation removal',\n",
    "        'Number removal', \n",
    "        'Stopword removal',\n",
    "        'Lemmatization'\n",
    "    ]\n",
    "}\n",
    "\n",
    "metadata_path = f'{models_dir}/model_metadata.pickle'\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"‚úì Metadata saved to: {metadata_path}\")\n",
    "\n",
    "# Create prediction function\n",
    "def predict_fake_news(text, model_path=model_path, vectorizer_path=vectorizer_path):\n",
    "    \\\"\\\"\\\"\n",
    "    Predict if a news article is fake or real.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The news article text to classify\n",
    "        model_path (str): Path to the saved model\n",
    "        vectorizer_path (str): Path to the saved TF-IDF vectorizer\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results containing label, probability, and confidence\n",
    "    \\\"\\\"\\\"\n",
    "    # Load model and vectorizer\n",
    "    model = joblib.load(model_path)\n",
    "    vectorizer = joblib.load(vectorizer_path)\n",
    "    \n",
    "    # Preprocess the text (same as training preprocessing)\n",
    "    import re\n",
    "    import string\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    # Initialize preprocessing tools\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Preprocess text\n",
    "    processed_text = text.lower()\n",
    "    processed_text = re.sub(r'http\\\\S+|www\\\\S+|https\\\\S+', '', processed_text, flags=re.MULTILINE)\n",
    "    processed_text = re.sub(r'@\\\\w+|#\\\\w+', '', processed_text)\n",
    "    processed_text = re.sub(r'\\\\d+', '', processed_text)\n",
    "    processed_text = processed_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    processed_text = re.sub(r'\\\\s+', ' ', processed_text).strip()\n",
    "    \n",
    "    tokens = word_tokenize(processed_text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens \n",
    "              if word not in stop_words and len(word) > 2]\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    # Transform text using TF-IDF\n",
    "    text_features = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(text_features)[0]\n",
    "    prediction_proba = model.predict_proba(text_features)[0]\n",
    "    \n",
    "    # Prepare results\n",
    "    label = 'FAKE' if prediction == 0 else 'REAL'\n",
    "    fake_prob = prediction_proba[0]\n",
    "    real_prob = prediction_proba[1]\n",
    "    confidence = max(fake_prob, real_prob)\n",
    "    \n",
    "    return {\n",
    "        'label': label,\n",
    "        'prediction': int(prediction),\n",
    "        'fake_probability': fake_prob,\n",
    "        'real_probability': real_prob,\n",
    "        'confidence': confidence,\n",
    "        'processed_text': processed_text[:200] + '...' if len(processed_text) > 200 else processed_text\n",
    "    }\n",
    "\n",
    "# Test the prediction function with sample texts\n",
    "print(f\\\"\\\\nTesting prediction function with sample texts:\\\")\n",
    "print(\\\"-\\\" * 60)\n",
    "\n",
    "# Test samples\n",
    "test_samples = [\n",
    "    \\\"Breaking: Scientists discover new cure for cancer using advanced AI technology.\\\",\n",
    "    \\\"SHOCKING: Aliens landed in Area 51 and government is hiding the truth!!!\\\",\n",
    "    \\\"The stock market closed higher today as investors remained optimistic about economic recovery.\\\",\n",
    "    \\\"You won't believe what this celebrity did! Doctors hate this one simple trick!\\\"\n",
    "]\n",
    "\n",
    "for i, sample_text in enumerate(test_samples, 1):\n",
    "    result = predict_fake_news(sample_text)\n",
    "    print(f\\\"\\\\nSample {i}:\\\")\n",
    "    print(f\\\"Text: {sample_text}\\\")\n",
    "    print(f\\\"Prediction: {result['label']} (Confidence: {result['confidence']:.3f})\\\")\n",
    "    print(f\\\"Probabilities - Fake: {result['fake_probability']:.3f}, Real: {result['real_probability']:.3f}\\\")\n",
    "\n",
    "print(f\\\"\\\\n‚úì Prediction function created and tested successfully!\\\")\n",
    "print(f\\\"\\\\nTo use the model in production:\\\")\n",
    "print(f\\\"1. Load the model: model = joblib.load('{model_path}')\\\")\n",
    "print(f\\\"2. Load the vectorizer: vectorizer = joblib.load('{vectorizer_path}')\\\")\n",
    "print(f\\\"3. Use predict_fake_news(text) function for predictions\\\")\n",
    "\n",
    "print(\\\"\\\\n\\\" + \\\"=\\\"*80)\n",
    "print(\\\"FAKE NEWS DETECTION PIPELINE COMPLETED SUCCESSFULLY!\\\")\n",
    "print(\\\"=\\\"*80)\n",
    "print(f\\\"\\\\nFinal Model Summary:\\\")\n",
    "print(f\\\"- Best Model: {best_model_name}\\\")\n",
    "print(f\\\"- F1-Score: {model_results[best_model_name]['f1']:.4f}\\\")\n",
    "print(f\\\"- Accuracy: {model_results[best_model_name]['accuracy']:.4f}\\\")\n",
    "print(f\\\"- Training Samples: {len(X_train):,}\\\")\n",
    "print(f\\\"- Test Samples: {len(X_test):,}\\\")\n",
    "print(f\\\"- Vocabulary Size: {len(tfidf_vectorizer.vocabulary_):,}\\\")\n",
    "print(f\\\"\\\\nModel files saved in: {models_dir}/\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
